"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[366],{8453(e,n,r){r.d(n,{R:()=>s,x:()=>c});var t=r(6540);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}},9893(e,n,r){r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"getting-started/ios-quickstart","title":"iOS Quickstart","description":"Build a simple pitch detector in 5 minutes.","source":"@site/docs/getting-started/ios-quickstart.md","sourceDirName":"getting-started","slug":"/getting-started/ios-quickstart","permalink":"/voxatrace/getting-started/ios-quickstart","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docsSidebar","previous":{"title":"Android Quickstart","permalink":"/voxatrace/getting-started/android-quickstart"},"next":{"title":"Pitch Detection","permalink":"/voxatrace/concepts/pitch-detection"}}');var i=r(4848),o=r(8453);const s={sidebar_position:3},c="iOS Quickstart",a={},l=[{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Configure Info.plist",id:"step-1-configure-infoplist",level:2},{value:"Step 2: Create the Pitch Detector View",id:"step-2-create-the-pitch-detector-view",level:2},{value:"Step 3: Create the View Model",id:"step-3-create-the-view-model",level:2},{value:"What You&#39;ll See",id:"what-youll-see",level:2},{value:"Step 4: Request Microphone Permission",id:"step-4-request-microphone-permission",level:2},{value:"Complete UIKit Example",id:"complete-uikit-example",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"ios-quickstart",children:"iOS Quickstart"})}),"\n",(0,i.jsx)(n.p,{children:"Build a simple pitch detector in 5 minutes."}),"\n",(0,i.jsx)(n.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,i.jsx)(n.p,{children:"A minimal SwiftUI app that:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Records audio from the microphone"}),"\n",(0,i.jsx)(n.li,{children:"Detects pitch in real-time"}),"\n",(0,i.jsx)(n.li,{children:"Displays the detected note"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Xcode 14 or later"}),"\n",(0,i.jsxs)(n.li,{children:["VoxaTrace installed (see ",(0,i.jsx)(n.a,{href:"./installation",children:"Installation"}),")"]}),"\n",(0,i.jsx)(n.li,{children:"Microphone permission configured in Info.plist"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"step-1-configure-infoplist",children:"Step 1: Configure Info.plist"}),"\n",(0,i.jsx)(n.p,{children:"Add microphone usage description:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"<key>NSMicrophoneUsageDescription</key>\r\n<string>We need microphone access to detect pitch from your voice.</string>\n"})}),"\n",(0,i.jsx)(n.h2,{id:"step-2-create-the-pitch-detector-view",children:"Step 2: Create the Pitch Detector View"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-swift",children:'import SwiftUI\r\nimport VoxaTrace\r\n\r\nstruct PitchDetectorView: View {\r\n    @StateObject private var viewModel = PitchDetectorViewModel()\r\n\r\n    var body: some View {\r\n        VStack(spacing: 20) {\r\n            Text(viewModel.note)\r\n                .font(.system(size: 72, weight: .bold))\r\n\r\n            Text("\\(Int(viewModel.frequency)) Hz")\r\n                .font(.title2)\r\n\r\n            ProgressView(value: viewModel.confidence)\r\n                .frame(width: 200)\r\n\r\n            Button(viewModel.isRecording ? "Stop" : "Start") {\r\n                if viewModel.isRecording {\r\n                    viewModel.stop()\r\n                } else {\r\n                    Task {\r\n                        await viewModel.start()\r\n                    }\r\n                }\r\n            }\r\n            .buttonStyle(.borderedProminent)\r\n        }\r\n        .padding()\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"step-3-create-the-view-model",children:"Step 3: Create the View Model"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-swift",children:'import Foundation\r\nimport VoxaTrace\r\nimport Combine\r\n\r\n@MainActor\r\nclass PitchDetectorViewModel: ObservableObject {\r\n    @Published var note: String = "--"\r\n    @Published var frequency: Float = 0\r\n    @Published var confidence: Float = 0\r\n    @Published var isRecording: Bool = false\r\n\r\n    private var recorder: SonixRecorder?\r\n    private var detector: CalibraPitch.Detector?\r\n    private var task: Task<Void, Never>?\r\n\r\n    func start() async {\r\n        // Create recorder with voice settings\r\n        recorder = SonixRecorder.companion.createTemporary(\r\n            config: SonixRecorderConfig.companion.VOICE\r\n        )\r\n\r\n        // Create pitch detector\r\n        detector = CalibraPitch.companion.createDetector()\r\n\r\n        // Start recording\r\n        recorder?.start()\r\n        isRecording = true\r\n\r\n        // Process audio buffers\r\n        task = Task {\r\n            guard let recorder = recorder else { return }\r\n\r\n            for await buffer in recorder.audioBuffersStream() {\r\n                guard !Task.isCancelled else { break }\r\n\r\n                let samples = buffer.toFloatArray()\r\n                guard let point = detector?.detect(samples: samples, sampleRate: Int32(buffer.sampleRate)) else {\r\n                    continue\r\n                }\r\n\r\n                await MainActor.run {\r\n                    if point.pitch > 0 {\r\n                        self.note = pitchToNote(point.pitch)\r\n                        self.frequency = point.pitch\r\n                        self.confidence = point.confidence\r\n                    } else {\r\n                        self.note = "--"\r\n                        self.frequency = 0\r\n                        self.confidence = 0\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    func stop() {\r\n        task?.cancel()\r\n        task = nil\r\n        recorder?.stop()\r\n        recorder?.release()\r\n        recorder = nil\r\n        detector?.close()\r\n        detector = nil\r\n        isRecording = false\r\n    }\r\n\r\n    private func pitchToNote(_ frequency: Float) -> String {\r\n        let noteNames = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]\r\n        let a4: Float = 440.0\r\n        let semitones = 12 * log2(frequency / a4) + 69\r\n        let noteIndex = (Int(semitones) % 12 + 12) % 12\r\n        let octave = Int(semitones) / 12 - 1\r\n        return "\\(noteNames[noteIndex])\\(octave)"\r\n    }\r\n\r\n    deinit {\r\n        stop()\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"what-youll-see",children:"What You'll See"}),"\n",(0,i.jsx)(n.p,{children:"When you sing into the microphone, the console will show:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Note: A4, Frequency: 440 Hz, Confidence: 92%\r\nNote: A4, Frequency: 441 Hz, Confidence: 89%\r\nNote: B4, Frequency: 494 Hz, Confidence: 87%\r\nNote: --, Frequency: 0 Hz, Confidence: 0%   \u2190 breath/silence\r\nNote: C5, Frequency: 523 Hz, Confidence: 91%\n"})}),"\n",(0,i.jsx)(n.p,{children:"The app is:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Recording audio buffers from the microphone (~50ms chunks)"}),"\n",(0,i.jsx)(n.li,{children:"Running pitch detection on each buffer"}),"\n",(0,i.jsx)(n.li,{children:"Converting frequency to musical note name"}),"\n",(0,i.jsx)(n.li,{children:"Showing confidence (how certain the detection is)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Troubleshooting:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Seeing lots of ",(0,i.jsx)(n.code,{children:"--"})," entries? Make sure microphone permission is granted in Settings"]}),"\n",(0,i.jsx)(n.li,{children:"Low confidence values? Sing closer to the device, reduce background noise"}),"\n",(0,i.jsxs)(n.li,{children:["App crashes on launch? Check that ",(0,i.jsx)(n.code,{children:"NSMicrophoneUsageDescription"})," is in Info.plist"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"step-4-request-microphone-permission",children:"Step 4: Request Microphone Permission"}),"\n",(0,i.jsx)(n.p,{children:"The system will automatically prompt for permission when you start recording. However, you can request it proactively:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-swift",children:"import AVFoundation\r\n\r\nfunc requestMicrophonePermission() async -> Bool {\r\n    await withCheckedContinuation { continuation in\r\n        AVAudioSession.sharedInstance().requestRecordPermission { granted in\r\n            continuation.resume(returning: granted)\r\n        }\r\n    }\r\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:"Use it in your view:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-swift",children:'struct PitchDetectorView: View {\r\n    @StateObject private var viewModel = PitchDetectorViewModel()\r\n    @State private var hasPermission = false\r\n\r\n    var body: some View {\r\n        Group {\r\n            if hasPermission {\r\n                // ... pitch detector UI\r\n            } else {\r\n                Button("Grant Microphone Access") {\r\n                    Task {\r\n                        hasPermission = await requestMicrophonePermission()\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        .task {\r\n            hasPermission = AVAudioSession.sharedInstance().recordPermission == .granted\r\n        }\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"complete-uikit-example",children:"Complete UIKit Example"}),"\n",(0,i.jsx)(n.p,{children:"If you're using UIKit:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-swift",children:'import UIKit\r\nimport VoxaTrace\r\n\r\nclass PitchDetectorViewController: UIViewController {\r\n    private let noteLabel = UILabel()\r\n    private let frequencyLabel = UILabel()\r\n    private let confidenceBar = UIProgressView()\r\n\r\n    private var recorder: SonixRecorder?\r\n    private var detector: CalibraPitch.Detector?\r\n    private var task: Task<Void, Never>?\r\n\r\n    override func viewDidLoad() {\r\n        super.viewDidLoad()\r\n        setupUI()\r\n    }\r\n\r\n    override func viewDidAppear(_ animated: Bool) {\r\n        super.viewDidAppear(animated)\r\n        Task {\r\n            await startPitchDetection()\r\n        }\r\n    }\r\n\r\n    override func viewWillDisappear(_ animated: Bool) {\r\n        super.viewWillDisappear(animated)\r\n        stopPitchDetection()\r\n    }\r\n\r\n    private func setupUI() {\r\n        view.backgroundColor = .systemBackground\r\n\r\n        noteLabel.font = .systemFont(ofSize: 72, weight: .bold)\r\n        noteLabel.textAlignment = .center\r\n        noteLabel.text = "--"\r\n\r\n        frequencyLabel.font = .systemFont(ofSize: 24)\r\n        frequencyLabel.textAlignment = .center\r\n        frequencyLabel.text = "0 Hz"\r\n\r\n        confidenceBar.progressViewStyle = .default\r\n\r\n        let stack = UIStackView(arrangedSubviews: [noteLabel, frequencyLabel, confidenceBar])\r\n        stack.axis = .vertical\r\n        stack.spacing = 16\r\n        stack.translatesAutoresizingMaskIntoConstraints = false\r\n\r\n        view.addSubview(stack)\r\n        NSLayoutConstraint.activate([\r\n            stack.centerXAnchor.constraint(equalTo: view.centerXAnchor),\r\n            stack.centerYAnchor.constraint(equalTo: view.centerYAnchor),\r\n            confidenceBar.widthAnchor.constraint(equalToConstant: 200)\r\n        ])\r\n    }\r\n\r\n    private func startPitchDetection() async {\r\n        recorder = SonixRecorder.companion.createTemporary(\r\n            config: SonixRecorderConfig.companion.VOICE\r\n        )\r\n        detector = CalibraPitch.companion.createDetector()\r\n\r\n        recorder?.start()\r\n\r\n        task = Task {\r\n            guard let recorder = recorder else { return }\r\n\r\n            for await buffer in recorder.audioBuffersStream() {\r\n                guard !Task.isCancelled else { break }\r\n\r\n                let samples = buffer.toFloatArray()\r\n                guard let point = detector?.detect(samples: samples, sampleRate: Int32(buffer.sampleRate)) else {\r\n                    continue\r\n                }\r\n\r\n                await MainActor.run {\r\n                    if point.pitch > 0 {\r\n                        self.noteLabel.text = self.pitchToNote(point.pitch)\r\n                        self.frequencyLabel.text = "\\(Int(point.pitch)) Hz"\r\n                        self.confidenceBar.progress = point.confidence\r\n                    } else {\r\n                        self.noteLabel.text = "--"\r\n                        self.frequencyLabel.text = "0 Hz"\r\n                        self.confidenceBar.progress = 0\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    private func stopPitchDetection() {\r\n        task?.cancel()\r\n        recorder?.stop()\r\n        recorder?.release()\r\n        detector?.close()\r\n    }\r\n\r\n    private func pitchToNote(_ frequency: Float) -> String {\r\n        let noteNames = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]\r\n        let a4: Float = 440.0\r\n        let semitones = 12 * log2(frequency / a4) + 69\r\n        let noteIndex = (Int(semitones) % 12 + 12) % 12\r\n        let octave = Int(semitones) / 12 - 1\r\n        return "\\(noteNames[noteIndex])\\(octave)"\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"../guides/detecting-pitch",children:"Detecting Pitch Guide"})," - Deep dive into pitch detection options"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"../guides/recording-audio",children:"Recording Audio Guide"})," - Learn about recording features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"../guides/live-evaluation",children:"Live Evaluation Guide"})," - Score singing against reference"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);