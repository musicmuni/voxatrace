"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[1272],{371(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>c,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"concepts/voice-activity","title":"Voice Activity Detection","description":"Voice Activity Detection (VAD) determines when someone is speaking or singing versus when there\'s silence or background noise.","source":"@site/docs/concepts/voice-activity.md","sourceDirName":"concepts","slug":"/concepts/voice-activity","permalink":"/concepts/voice-activity","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Pitch Detection","permalink":"/concepts/pitch-detection"},"next":{"title":"Live Evaluation","permalink":"/concepts/live-evaluation"}}');var r=i(4848),t=i(8453);const c={sidebar_position:2},d="Voice Activity Detection",l={},o=[{value:"What is VAD?",id:"what-is-vad",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Backends",id:"backends",level:2},{value:"General (Energy-based)",id:"general-energy-based",level:3},{value:"Speech (Silero VAD)",id:"speech-silero-vad",level:3},{value:"Singing Realtime (SwiftF0-based)",id:"singing-realtime-swiftf0-based",level:3},{value:"API Modes",id:"api-modes",level:2},{value:"Batch Mode",id:"batch-mode",level:3},{value:"Streaming Mode",id:"streaming-mode",level:3},{value:"VAD Result",id:"vad-result",level:2},{value:"Threshold Tuning",id:"threshold-tuning",level:2},{value:"Common Patterns",id:"common-patterns",level:2},{value:"Auto-Record When Voice Detected",id:"auto-record-when-voice-detected",level:3},{value:"Skip Silent Sections for Analysis",id:"skip-silent-sections-for-analysis",level:3},{value:"Combine with Pitch Detection",id:"combine-with-pitch-detection",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Next Steps",id:"next-steps",level:2}];function a(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"voice-activity-detection",children:"Voice Activity Detection"})}),"\n",(0,r.jsx)(n.p,{children:"Voice Activity Detection (VAD) determines when someone is speaking or singing versus when there's silence or background noise."}),"\n",(0,r.jsx)(n.h2,{id:"what-is-vad",children:"What is VAD?"}),"\n",(0,r.jsxs)(n.p,{children:["VAD answers a simple question: ",(0,r.jsx)(n.strong,{children:'"Is there voice in this audio?"'})]}),"\n",(0,r.jsxs)(n.p,{children:["Unlike pitch detection (which tells you ",(0,r.jsx)(n.em,{children:"what note"})," is being sung), VAD tells you ",(0,r.jsx)(n.em,{children:"whether someone is singing at all"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Scenario"}),(0,r.jsx)(n.th,{children:"How VAD Helps"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Recording app"})}),(0,r.jsx)(n.td,{children:"Auto-start/stop recording when voice is detected"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Transcription"})}),(0,r.jsx)(n.td,{children:"Skip silent sections to save processing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Singing evaluation"})}),(0,r.jsx)(n.td,{children:"Only score segments where the user is singing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Noise gate"})}),(0,r.jsx)(n.td,{children:"Mute output when no voice is present"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Voice commands"})}),(0,r.jsx)(n.td,{children:"Detect when user starts speaking"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,r.jsx)(n.p,{children:"VAD analyzes audio and returns:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice Detected"})," (boolean): Is voice present right now?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VAD Ratio"})," (0.0 to 1.0): What percentage of the audio contains voice?"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"backends",children:"Backends"}),"\n",(0,r.jsx)(n.p,{children:"VoxaTrace offers four VAD backends optimized for different scenarios:"}),"\n",(0,r.jsx)(n.h3,{id:"general-energy-based",children:"General (Energy-based)"}),"\n",(0,r.jsx)(n.p,{children:"Simple and fast. Detects when audio exceeds an energy threshold."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Details"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Speed"})}),(0,r.jsx)(n.td,{children:"Very fast"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Accuracy"})}),(0,r.jsx)(n.td,{children:"Basic"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Dependencies"})}),(0,r.jsx)(n.td,{children:"None"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Best For"})}),(0,r.jsx)(n.td,{children:"Simple voice detection, low-resource devices"})]})]})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val vad = CalibraVAD.create(VADModelProvider.General)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"speech-silero-vad",children:"Speech (Silero VAD)"}),"\n",(0,r.jsx)(n.p,{children:"Neural network trained on speech. Very accurate for spoken content."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Details"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Speed"})}),(0,r.jsx)(n.td,{children:"Fast"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Accuracy"})}),(0,r.jsx)(n.td,{children:"Excellent for speech"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Dependencies"})}),(0,r.jsx)(n.td,{children:"ONNX Runtime"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Best For"})}),(0,r.jsx)(n.td,{children:"Voice assistants, transcription"})]})]})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val vad = CalibraVAD.create(\r\n    VADModelProvider.Speech { ModelLoader.loadSpeechVAD() }\r\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"singing-realtime-swiftf0-based",children:"Singing Realtime (SwiftF0-based)"}),"\n",(0,r.jsx)(n.p,{children:"Uses pitch detection confidence for low-latency singing detection."}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Details"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Speed"})}),(0,r.jsx)(n.td,{children:"Very fast"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Accuracy"})}),(0,r.jsx)(n.td,{children:"Good for singing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Dependencies"})}),(0,r.jsx)(n.td,{children:"ONNX Runtime"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Best For"})}),(0,r.jsx)(n.td,{children:"Real-time singing apps, low latency"})]})]})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val vad = CalibraVAD.create(\r\n    VADModelProvider.SingingRealtime { ModelLoader.loadSingingRealtimeVAD() }\r\n)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"api-modes",children:"API Modes"}),"\n",(0,r.jsx)(n.h3,{id:"batch-mode",children:"Batch Mode"}),"\n",(0,r.jsx)(n.p,{children:"Analyze a complete audio segment:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:'val vad = CalibraVAD.create(VADModelProvider.General)\r\n\r\n// Get ratio of voiced frames (0.0 = all silence, 1.0 = all voice)\r\nval ratio = vad.getVADRatio(samples, sampleRate = 48000)\r\n\r\n// Get rich result with level classification\r\nval result = vad.analyze(samples, sampleRate = 48000)\r\nprintln("Ratio: ${result?.ratio}, Level: ${result?.level}")\r\n\r\nvad.release()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"streaming-mode",children:"Streaming Mode"}),"\n",(0,r.jsx)(n.p,{children:"For real-time processing, feed audio continuously:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val vad = CalibraVAD.create(VADModelProvider.Speech { ... })\r\n\r\nrecorder.audioBuffers.collect { buffer ->\r\n    vad.acceptWaveform(buffer.toFloatArray(), buffer.sampleRate)\r\n\r\n    if (vad.isVoiceDetected()) {\r\n        showVoiceIndicator()\r\n    } else {\r\n        hideVoiceIndicator()\r\n    }\r\n}\r\n\r\nvad.release()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"vad-result",children:"VAD Result"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"analyze()"})," method returns a ",(0,r.jsx)(n.code,{children:"VADResult"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"data class VADResult(\r\n    val ratio: Float,              // 0.0 to 1.0\r\n    val level: VoiceActivityLevel, // NONE, PARTIAL, FULL\r\n    val isVoiceDetected: Boolean,  // ratio > 0.3\r\n    val isFullActivity: Boolean    // ratio > 0.7\r\n)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Use the level for UI feedback:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"when (result.level) {\r\n    VoiceActivityLevel.NONE -> showSilenceState()\r\n    VoiceActivityLevel.PARTIAL -> showWeakVoice()\r\n    VoiceActivityLevel.FULL -> showStrongVoice()\r\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"threshold-tuning",children:"Threshold Tuning"}),"\n",(0,r.jsxs)(n.p,{children:["Adjust sensitivity with ",(0,r.jsx)(n.code,{children:"VADConfig"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"val config = VADConfig.Builder()\r\n    .preset(VADConfig.SPEECH)\r\n    .threshold(0.3f)  // Lower = more sensitive (more false positives)\r\n    .build()\r\n\r\nval vad = CalibraVAD.create(config, VADModelProvider.Speech { ... })\n"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Threshold"}),(0,r.jsx)(n.th,{children:"Behavior"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0.2"}),(0,r.jsx)(n.td,{children:"Very sensitive, catches faint voice"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0.4"}),(0,r.jsx)(n.td,{children:"Balanced (default for Speech)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0.6"}),(0,r.jsx)(n.td,{children:"Strict, requires clear voice"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0.8"}),(0,r.jsx)(n.td,{children:"Very strict, only loud/clear voice"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"common-patterns",children:"Common Patterns"}),"\n",(0,r.jsx)(n.h3,{id:"auto-record-when-voice-detected",children:"Auto-Record When Voice Detected"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"var isRecording = false\r\n\r\nrecorder.audioBuffers.collect { buffer ->\r\n    vad.acceptWaveform(buffer.toFloatArray(), buffer.sampleRate)\r\n\r\n    if (vad.isVoiceDetected() && !isRecording) {\r\n        isRecording = true\r\n        startRecording()\r\n    } else if (!vad.isVoiceDetected() && isRecording) {\r\n        // Add debounce to avoid cutting off between words\r\n        isRecording = false\r\n        stopRecording()\r\n    }\r\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"skip-silent-sections-for-analysis",children:"Skip Silent Sections for Analysis"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:'val contour = pitchExtractor.extract(audio, sampleRate)\r\nval vadRatio = vad.getVADRatio(audio, sampleRate)\r\n\r\nif (vadRatio < 0.1f) {\r\n    // Mostly silence, skip scoring\r\n    return SkipResult("No voice detected")\r\n}\r\n\r\n// Proceed with evaluation\r\nval score = evaluator.evaluate(contour)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"combine-with-pitch-detection",children:"Combine with Pitch Detection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-kotlin",children:"recorder.audioBuffers.collect { buffer ->\r\n    val samples = buffer.toFloatArray()\r\n\r\n    // Quick VAD check first (cheap)\r\n    vad.acceptWaveform(samples, buffer.sampleRate)\r\n    if (!vad.isVoiceDetected()) {\r\n        updateUI(pitch = null)  // Show silence\r\n        return@collect\r\n    }\r\n\r\n    // Only run pitch detection if voice detected (expensive)\r\n    val point = detector.detect(samples, buffer.sampleRate)\r\n    updateUI(pitch = point)\r\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Backend"}),(0,r.jsx)(n.th,{children:"Latency"}),(0,r.jsx)(n.th,{children:"Memory"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"General"}),(0,r.jsx)(n.td,{children:"< 1ms"}),(0,r.jsx)(n.td,{children:"Minimal"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Speech"}),(0,r.jsx)(n.td,{children:"~5ms"}),(0,r.jsx)(n.td,{children:"~10 MB (model)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Singing"}),(0,r.jsx)(n.td,{children:"~20ms"}),(0,r.jsx)(n.td,{children:"~50 MB (model)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"SingingRealtime"}),(0,r.jsx)(n.td,{children:"~5ms"}),(0,r.jsx)(n.td,{children:"~10 MB (model)"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:["For battery-sensitive apps, use ",(0,r.jsx)(n.strong,{children:"General"})," backend and only load neural models when needed."]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"./pitch-detection",children:"Pitch Detection"})," - Detect what note is being sung"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"./live-evaluation",children:"Live Evaluation"})," - Score singing in real-time"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453(e,n,i){i.d(n,{R:()=>c,x:()=>d});var s=i(6540);const r={},t=s.createContext(r);function c(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);